{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the kitti dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Kitti\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def custom_collate_fn(data):\n",
    "  return tuple(zip(*data))\n",
    "data_path = \"./train/\"\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # hint: there is something missin here\n",
    "    # change the size of the image to 3x224x224\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "\n",
    "    ])\n",
    "\n",
    "train_dataset = Kitti(\n",
    "    data_path,\n",
    "    train= True, # hint: its a boolean\n",
    "    transform=transform,\n",
    "    download=True\n",
    "    )\n",
    "\n",
    "test_dataset = Kitti(\n",
    "    data_path,\n",
    "    train= False, # hint: its a boolean\n",
    "    transform=transform,\n",
    "    download=True\n",
    "    )\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiti_classes = {'Car': 0,\n",
    " 'Cyclist': 1,\n",
    " 'DontCare': 2,\n",
    " 'Misc': 3,\n",
    " 'Pedestrian': 4,\n",
    " 'Person_sitting': 5,\n",
    " 'Tram': 6,\n",
    " 'Truck': 7,\n",
    " 'Van': 8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing the mobilenet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  7481\n"
     ]
    }
   ],
   "source": [
    "# travers all the data to check the number of classes\n",
    "classes = []\n",
    "for i in range(len(train_dataset)):\n",
    "  classes.append(train_dataset[i][1])\n",
    "# classes = set(classes)\n",
    "print(\"Number of classes: \", len(classes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pedestrian', 'Truck', 'Misc', 'Car']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = [cls[0]['type'] for cls in classes]\n",
    "classes[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  8\n"
     ]
    }
   ],
   "source": [
    "clset = set(classes)\n",
    "print(\"Number of classes: \", len(clset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:  torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# check size of the image\n",
    "img = train_dataset[0][0]\n",
    "print(\"Image size: \", img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trh00\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\trh00\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layer except the last one\n",
    "for param in mobilenet_v2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# change the last layer to fit our problem\n",
    "mobilenet_v2.classifier[1] = torch.nn.Linear(1280, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilenet_v2.to(device)\n",
    "optimizer = torch.optim.Adam(mobilenet_v2.parameters(), lr=0.001)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/117 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "values expected sparse tensor layout but got Strided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m mobilenet_v2(images)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 29\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m \u001b[43mloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Sum all the different losses.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     32\u001b[0m losses\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: values expected sparse tensor layout but got Strided"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(num_epochs):\n",
    "    mobilenet_v2.train()\n",
    "    for images, targets in tqdm(train_loader):\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        images = torch.stack(images).to(device)  # Shape: [batch_size, 3, height, width]\n",
    "        new_targets = []\n",
    "        for target in targets:\n",
    "          boxes = []\n",
    "          labels = []\n",
    "          for obj in target:\n",
    "            label = obj['type']\n",
    "            box = obj['bbox']\n",
    "            # xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "            box = torch.Tensor(box).to(device)\n",
    "            boxes.append(box)\n",
    "            labels.append(kiti_classes[label])\n",
    "          target = {}\n",
    "          target['boxes'] = torch.stack(boxes)\n",
    "          target['labels'] = torch.Tensor(labels).type(torch.int64).to(device)\n",
    "          new_targets.append(target)\n",
    "\n",
    "\n",
    "        # In training mode, the model will take images and targets and calculate all the losses in internal implementation\n",
    "        output = mobilenet_v2(images)\n",
    "        print(output)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())  # Sum all the different losses.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {losses.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
